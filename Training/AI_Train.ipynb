{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zRBRDjy6yHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### required\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pN4kSXxTTrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### required\n",
        "!pip install configparser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpLxAyOZ-kIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### required\n",
        "!pip install keras==1.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7MHekXM5YVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### required\n",
        "!pip install path.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DBpa9GO7bQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Reshape\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, adam, Adagrad\n",
        "from scipy.io import loadmat, savemat\n",
        "from keras.models import model_from_json\n",
        "import theano.tensor as T\n",
        "import tensorflow as tf\n",
        "import theano\n",
        "import configparser\n",
        "import collections\n",
        "import time\n",
        "import path\n",
        "import os\n",
        "import skimage.transform\n",
        "from skimage import color\n",
        "import numpy as np\n",
        "import numpy\n",
        "from datetime import datetime\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX5dMBbH8e0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, json_path, weight_path):\n",
        "    json_string = model.to_json()\n",
        "    open(json_path, 'w').write(json_string)\n",
        "    dict = {}\n",
        "    i = 0\n",
        "    for layer in model.layers:\n",
        "        weights = layer.get_weights()\n",
        "        my_list = np.zeros(len(weights), dtype=np.object)\n",
        "        my_list[:] = weights\n",
        "        dict[str(i)] = my_list\n",
        "        i += 1\n",
        "    savemat(weight_path, dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-EjppUh3eS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(json_path):  # Function to load the model\n",
        "    model = model_from_json(open(json_path).read())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XElHujGq8sgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset_Train_batch(AbnormalPath, NormalPath):\n",
        "  \n",
        "    batchsize=60\n",
        "    n_exp= int(batchsize/2)\n",
        "\n",
        "    Num_abnormal = 900\n",
        "    Num_Normal = 792.\n",
        "\n",
        "\n",
        "    Abnor_list_iter = np.random.permutation(Num_abnormal)\n",
        "    Abnor_list_iter = Abnor_list_iter[Num_abnormal-n_exp:]\n",
        "    Norm_list_iter = np.random.permutation(Num_Normal)\n",
        "    Norm_list_iter = Norm_list_iter[Num_Normal-n_exp:]\n",
        "    \n",
        "    All_Videos=[]\n",
        "    with open(AbnormalPath+\"/anomaly.txt\", 'r') as f1: #file contain path to anomaly video file.\n",
        "      for line in f1:\n",
        "          All_Videos.append(line.strip())\n",
        "    AllFeatures = []\n",
        "    print(\"Loading Anomaly videos Features...\")\n",
        "\n",
        "    Video_count=-1\n",
        "    for iv in Abnor_list_iter:\n",
        "        Video_count=Video_count+1\n",
        "        VideoPath = os.path.join(AbnormalPath, All_Videos[iv])\n",
        "        f = open(VideoPath, \"r\")\n",
        "        words = f.read().split()\n",
        "        num_feat = len(words) / 4096\n",
        "        \n",
        "        count = -1;\n",
        "        VideoFeatues = []\n",
        "        for feat in range(0, int(num_feat)):\n",
        "            feat_row1 = np.float32(words[feat * 4096:feat * 4096 + 4096])\n",
        "            count = count + 1\n",
        "            if count == 0:\n",
        "                VideoFeatues = feat_row1\n",
        "            if count > 0:\n",
        "                VideoFeatues = np.vstack((VideoFeatues, feat_row1))\n",
        "\n",
        "        if Video_count == 0:\n",
        "            AllFeatures = VideoFeatues\n",
        "        if Video_count > 0:\n",
        "            AllFeatures = np.vstack((AllFeatures, VideoFeatues))\n",
        "    print(\" Abnormal Features  loaded\")\n",
        "\n",
        "    All_Videos=[]\n",
        "    with open(NormalPath+\"/normal.txt\", 'r') as f1: #file contain path to normal video file.\n",
        "        for line in f1:\n",
        "            All_Videos.append(line.strip())\n",
        "    \n",
        "    print(\"Loading Normal videos...\")\n",
        "  \n",
        "    for iv in Norm_list_iter:\n",
        "        VideoPath = os.path.join(NormalPath, All_Videos[iv])\n",
        "        f = open(VideoPath, \"r\")\n",
        "        words = f.read().split()\n",
        "        feat_row1 = np.array([])\n",
        "        num_feat = len(words) /4096\n",
        "        count = -1;\n",
        "        VideoFeatues = []\n",
        "        for feat in range(0, int(num_feat)):\n",
        "            feat_row1 = np.float32(words[feat * 4096:feat * 4096 + 4096])\n",
        "            count = count + 1\n",
        "            if count == 0:\n",
        "                VideoFeatues = feat_row1\n",
        "            if count > 0:\n",
        "                VideoFeatues = np.vstack((VideoFeatues, feat_row1))\n",
        "            feat_row1 = []\n",
        "        AllFeatures = np.vstack((AllFeatures, VideoFeatues))\n",
        "\n",
        "    print(\"Features  loaded\")\n",
        "\n",
        "    AllLabels = np.zeros(32*batchsize, dtype='uint8')\n",
        "    th_loop1=n_exp*32\n",
        "    th_loop2=n_exp*32-1\n",
        "\n",
        "    for iv in range(0, 32*batchsize):\n",
        "            if iv< th_loop1:\n",
        "                AllLabels[iv] = int(0)\n",
        "            if iv > th_loop2:\n",
        "                AllLabels[iv] = int(1)\n",
        "\n",
        "    return  AllFeatures,AllLabels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDcDLqY321gJ",
        "colab_type": "text"
      },
      "source": [
        "Custom loss function - MIL ranking loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjvHozNG9L3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For custom loss function - ref = https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618. \n",
        "\n",
        "def custom_objective(y_true, y_pred):\n",
        "\n",
        "    y_true = T.flatten(y_true)\n",
        "    y_pred = T.flatten(y_pred)\n",
        "   \n",
        "    n_seg = 32\n",
        "    nvid = 60\n",
        "    n_exp = nvid / 2\n",
        "    Num_d=32*nvid\n",
        "\n",
        "    sub_max = T.ones_like(y_pred)\n",
        "    sub_sum_labels = T.ones_like(y_true)\n",
        "    sub_sum_l1=T.ones_like(y_true) \n",
        "    sub_l2 = T.ones_like(y_true)\n",
        "\n",
        "    for ii in range(0, nvid, 1):\n",
        "      \n",
        "        mm = y_true[ii * n_seg:ii * n_seg + n_seg]\n",
        "        sub_sum_labels = T.concatenate([sub_sum_labels, T.stack(T.sum(mm))])\n",
        "\n",
        "        Feat_Score = y_pred[ii * n_seg:ii * n_seg + n_seg]\n",
        "        sub_max = T.concatenate([sub_max, T.stack(T.max(Feat_Score))])  \n",
        "        sub_sum_l1 = T.concatenate([sub_sum_l1, T.stack(T.sum(Feat_Score))])\n",
        "\n",
        "        z1 = T.ones_like(Feat_Score)\n",
        "        z2 = T.concatenate([z1, Feat_Score])\n",
        "        z3 = T.concatenate([Feat_Score, z1])\n",
        "        z_22 = z2[31:]\n",
        "        z_44 = z3[:33]\n",
        "        z = z_22 - z_44\n",
        "        z = z[1:32]\n",
        "        z = T.sum(T.sqr(z))\n",
        "        sub_l2 = T.concatenate([sub_l2, T.stack(z)])\n",
        "\n",
        "\n",
        "    sub_score = sub_max[Num_d:]\n",
        "    F_labels = sub_sum_labels[Num_d:]\n",
        "    \n",
        "\n",
        "    sub_sum_l1 = sub_sum_l1[Num_d:]\n",
        "    sub_sum_l1 = sub_sum_l1[:n_exp]\n",
        "    sub_l2 = sub_l2[Num_d:]\n",
        "    sub_l2 = sub_l2[:n_exp]\n",
        "\n",
        "    indx_nor = theano.tensor.eq(F_labels, 32).nonzero()[0]\n",
        "    indx_abn = theano.tensor.eq(F_labels, 0).nonzero()[0]\n",
        "\n",
        "    n_Nor=n_exp\n",
        "\n",
        "    Sub_Nor = sub_score[indx_nor]\n",
        "    Sub_Abn = sub_score[indx_abn]\n",
        "\n",
        "    z = T.ones_like(y_true)\n",
        "    for ii in range(0, n_Nor, 1):\n",
        "        sub_z = T.maximum(1 - Sub_Abn + Sub_Nor[ii], 0)\n",
        "        z = T.concatenate([z, T.stack(T.sum(sub_z))])\n",
        "\n",
        "    z = z[Num_d:]\n",
        "    z = T.mean(z, axis=-1) +  0.00008*T.sum(sub_sum_l1) + 0.00008*T.sum(sub_l2)\n",
        "\n",
        "    return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5b_-jMv4yUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path contains C3D features (.txt file) of each video.\n",
        "# Each text file contains 32 features, each of 4096 dimension\n",
        "AllClassPath='/content/drive/My Drive/C3D/C3D-v1.0/examples/c3d_feature_extraction/out'\n",
        "\n",
        "output_dir='/content/'\n",
        "\n",
        "# Output_dir save trained weights and model.\n",
        "\n",
        "weights_path = output_dir + 'weights.mat'\n",
        "\n",
        "model_path = output_dir + 'model.json'\n",
        "\n",
        "#model=load_model(\"/content/drive/My Drive/C3D/C3D-v1.0/examples/c3d_feature_extraction/out/\"+model_path)\n",
        "\n",
        "#Create Full connected Model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=4096,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu'))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(32,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(1,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='sigmoid'))\n",
        "\n",
        "adagrad=Adagrad(lr=0.01, epsilon=1e-08)\n",
        "\n",
        "model.compile(loss=custom_objective, optimizer=adagrad)\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "       os.makedirs(output_dir)\n",
        "\n",
        "All_class_files.sort()\n",
        "loss_graph =[]\n",
        "num_iters = 20000\n",
        "total_iterations = 0\n",
        "batchsize=60\n",
        "time_before = datetime.now()\n",
        "\n",
        "for it_num in range(num_iters):\n",
        "    inputs, targets=load_dataset_Train_batch(AllClassPath, AllClassPath)\n",
        "    batch_loss =model.train_on_batch(inputs, targets)\n",
        "    loss_graph = np.hstack((loss_graph, batch_loss))\n",
        "    total_iterations += 1\n",
        "    if total_iterations % 20 == 1:\n",
        "        print (\"These iteration=\" + str(total_iterations) + \") took: \" + str(datetime.now() - time_before) + \", with loss of \" + str(batch_loss))\n",
        "\n",
        "save_model(model, model_path, weights_path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}